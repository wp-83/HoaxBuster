{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d52ec0b2",
   "metadata": {},
   "source": [
    "# DATA PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ce77e7",
   "metadata": {},
   "source": [
    "## IMPORT LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861c5029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from sklearn.utils import shuffle\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644c4c26",
   "metadata": {},
   "source": [
    "## RAW DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307da342",
   "metadata": {},
   "source": [
    "### GET ALL THE RAW DATA FROM DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83677ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path('../data/raw/base')\n",
    "\n",
    "# Make empty dictionary to set place for all of the data   \n",
    "datasets = {}\n",
    "\n",
    "# Loop all files and insert to the dictionary\n",
    "for file_name in os.listdir(base_dir):\n",
    "    if file_name.endswith('.xlsx'):\n",
    "        var_name = file_name.replace('.xlsx', '').lower()\n",
    "        datasets[var_name] = pd.read_excel(base_dir/file_name)\n",
    "\n",
    "# print(f\"Keys: {list(datasets.keys())}\") # list of all keys\n",
    "# datasets['dataset_cnn_10k']  # access specific file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ed9045",
   "metadata": {},
   "source": [
    "### MODIFIED CNN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ef8e2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 6)\n",
      "(9627, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prata\\AppData\\Local\\Temp\\ipykernel_20840\\4124802349.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clear_data['info'] = clear_data['title'] + clear_data['fulltext'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "cnn_data = datasets['dataset_cnn_10k']\n",
    "\n",
    "# Check shape and the sum of n/a data\n",
    "print(cnn_data.shape)\n",
    "cnn_data.isna().sum()\n",
    "# na_data = cnn_data[cnn_data.isna().any(axis=1)]\n",
    "\n",
    "# Remove n/a data\n",
    "clear_data = cnn_data.dropna()\n",
    "clear_data.isna().sum()\n",
    "print(clear_data.shape)\n",
    "\n",
    "# Modified the header into lowercase\n",
    "clear_data.columns = clear_data.columns.str.lower()\n",
    "\n",
    "# Remove unnecessary data and add 'hoax' label\n",
    "clear_data['info'] = clear_data['title'] + clear_data['fulltext'].astype(str)\n",
    "clear_data = clear_data.drop(['title', 'timestamp', 'fulltext', 'tags', 'author', 'url'], axis=1)\n",
    "clear_data['hoax'] = 0\n",
    "\n",
    "# Save the modified data into new csv file\n",
    "clear_data.to_csv('../data/raw/modified/cnn_modified.csv', index=False, sep=',', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e596cc",
   "metadata": {},
   "source": [
    "### MODIFIED KOMPAS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "467ef5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4750, 6)\n",
      "(4286, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prata\\AppData\\Local\\Temp\\ipykernel_20840\\1649812826.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clear_data['info'] = clear_data['title'] + clear_data['fulltext'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "kompas_data = datasets['dataset_kompas_4k']\n",
    "\n",
    "# Check shape and the sum of n/a data\n",
    "print(kompas_data.shape)\n",
    "kompas_data.isna().sum()\n",
    "\n",
    "# Remove n/a data\n",
    "clear_data = kompas_data.dropna()\n",
    "clear_data.isna().sum()\n",
    "print(clear_data.shape)\n",
    "\n",
    "# Modified the header into lowercase\n",
    "clear_data.columns = clear_data.columns.str.lower()\n",
    "\n",
    "# Remove unnecessary data and add 'hoax' label\n",
    "clear_data['info'] = clear_data['title'] + clear_data['fulltext'].astype(str)\n",
    "clear_data = clear_data.drop(['title', 'timestamp', 'fulltext', 'tags', 'author', 'url'], axis=1)\n",
    "clear_data['hoax'] = 0\n",
    "\n",
    "# Save the modified data into new csv file\n",
    "clear_data.to_csv('../data/raw/modified/kompas_modified.csv', index=False, sep=',', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed1ab09",
   "metadata": {},
   "source": [
    "### MODIFIED TEMPO DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c872a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6592, 6)\n",
      "(6591, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prata\\AppData\\Local\\Temp\\ipykernel_20840\\2532434727.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clear_data['info'] = clear_data['title'] + clear_data['fulltext'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "tempo_data = datasets['dataset_tempo_6k']\n",
    "\n",
    "print(tempo_data.shape)\n",
    "tempo_data.isna().sum()\n",
    "\n",
    "clear_data = tempo_data.dropna()\n",
    "clear_data.isna().sum()\n",
    "print(clear_data.shape)\n",
    "\n",
    "clear_data.columns = clear_data.columns.str.lower()\n",
    "\n",
    "clear_data['info'] = clear_data['title'] + clear_data['fulltext'].astype(str)\n",
    "clear_data = clear_data.drop(['title', 'fulltext', 'timestamp', 'tags', 'author', 'url'], axis=1)\n",
    "clear_data['hoax'] = 0\n",
    "\n",
    "clear_data.to_csv('../data/raw/modified/tempo_modified.csv', index=False, sep=',', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f006e6",
   "metadata": {},
   "source": [
    "### MODIFIED TURNBACKHOAX DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab7df22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10384, 6)\n",
      "(10381, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prata\\AppData\\Local\\Temp\\ipykernel_20840\\1064096877.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clear_data['info'] = clear_data['title'] + clear_data['fulltext'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "turnbackhoax_data = datasets['dataset_turnbackhoax_10k']\n",
    "\n",
    "print(turnbackhoax_data.shape)\n",
    "turnbackhoax_data.isna().sum()\n",
    "\n",
    "clear_data = turnbackhoax_data.dropna()\n",
    "clear_data.isna().sum()\n",
    "print(clear_data.shape)\n",
    "\n",
    "clear_data.columns = clear_data.columns.str.lower()\n",
    "\n",
    "clear_data['info'] = clear_data['title'] + clear_data['fulltext'].astype(str)\n",
    "clear_data = clear_data.drop(['title','fulltext', 'timestamp', 'tags', 'author', 'url'], axis=1)\n",
    "clear_data['hoax'] = 1\n",
    "\n",
    "clear_data.to_csv('../data/raw/modified/turnbackhoax_modified.csv', index=False, sep=',', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f5059a",
   "metadata": {},
   "source": [
    "## PROCESS THE MODIFIED DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28958f6",
   "metadata": {},
   "source": [
    "### GET ALL THE RAW DATA FROM DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9561fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path('../data/raw/modified')\n",
    "\n",
    "modified_data = {}\n",
    "\n",
    "for file_name in os.listdir(base_path):\n",
    "    if (file_name.endswith('.csv')):\n",
    "        key_name = file_name.replace('.csv', '').lower()\n",
    "        modified_data[key_name] = pd.read_csv(base_path/file_name)\n",
    "\n",
    "# print(list(modified_data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466e46f8",
   "metadata": {},
   "source": [
    "#### Function for Pre-Processing Data in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d74410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup tools\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Define additional stopwords that need\n",
    "custom_stopwords = {\n",
    "    'yg', 'dg', 'rt', 'dgn', 'ny', 'dll', 'tsb', 'dr', 'pd',\n",
    "    'scroll', 'resume', 'advertisement', 'iklan', 'sponsor', 'promo', 'baca',\n",
    "    'klik', 'lanjut', 'selengkapnya', 'like', 'share', 'comment', 'subscribe',\n",
    "    'follow', 'video', 'foto', 'gambar', 'infografis', 'caption', 'deskripsi',\n",
    "    'sumber', 'reporter', 'editor', 'wartawan', 'penulis', 'kontributor', 'publisher',\n",
    "    'simak', 'tonton', 'lihat', 'dengar', 'unduh', 'download',\n",
    "    'republika', 'kompas', 'detik', 'tempo', 'cnn', 'bbc', 'rt', 'via',\n",
    "    'twitter', 'facebook', 'instagram', 'youtube', 'tiktok',\n",
    "    'halaman', 'kategori', 'narasi', 'verifikasi', 'referensi',\n",
    "    'error', 'tagar', 'tulis', 'komen', 'read',\n",
    "    'www', 'https', 'http', 'com', 'net', 'co', 'id'\n",
    "}\n",
    "\n",
    "# merge custom stopwords and default stopwords\n",
    "stopwords = set(stopword_factory.get_stop_words()).union(custom_stopwords)\n",
    "\n",
    "# exclude some words that don't need to be stemmed\n",
    "excluded_from_stemming = {\n",
    "    'politik', 'ekonomi', 'tokoh', 'jakarta', 'indonesia', 'pemerintah',\n",
    "    'demokrasi', 'korupsi', 'hukum', 'budaya', 'sejarah', 'teknologi'\n",
    "}\n",
    "\n",
    "def preprocess_indonesian(text):\n",
    "    # ensure the data are text or string\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "\n",
    "    # case folding (lowercase + remove escape characters)\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # remove data that are not important\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+|\\S+\\.(com|id|net|org|co)(/\\S*)?)', '', text, flags=re.IGNORECASE) #url + domain\n",
    "    text = re.sub(r'\\b\\w+@\\w+\\.\\w+', '', text)  #email\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text) #symbols and punctuations  \n",
    "    text = re.sub(r'\\s+', ' ', text).strip() #space after several operations\n",
    "\n",
    "    # tokenize the string (sentences into words)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Filter the token from short token, stopwords, and remain words about URL\n",
    "    tokens = [\n",
    "        t for t in tokens \n",
    "        if len(t) > 3 and t not in stopwords and not re.match(r'^(http|https|www|com|net|co|id)$', t)\n",
    "    ]\n",
    "\n",
    "    # Stemming the words (only take the base form of a word)\n",
    "    tokens = [t if t in excluded_from_stemming else stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    #return clean string\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66f469b",
   "metadata": {},
   "source": [
    "### CLEANING CNN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0d38b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9627, 2)\n",
      "(9627, 2)\n"
     ]
    }
   ],
   "source": [
    "cnn_modified = modified_data['cnn_modified']\n",
    "print(cnn_modified.shape)\n",
    "\n",
    "# Remove whitespace and duplicate title value\n",
    "cnn_modified['info'] = cnn_modified['info'].str.strip()\n",
    "cnn_modified = cnn_modified.drop_duplicates(subset=['info'])\n",
    "print(cnn_modified.shape)\n",
    "\n",
    "# Re-calculate data index after remove duplicate value if any\n",
    "cnn_modified = cnn_modified.reset_index(drop=True)\n",
    "\n",
    "# Apply preprocessing function to all of the data\n",
    "cnn_modified['cleaned_info'] = cnn_modified['info'].apply(preprocess_indonesian)\n",
    "\n",
    "# Show results\n",
    "# print(\"Original:\", cnn_modified['title'].iloc[5])\n",
    "# print(\"Cleaned:\", cnn_modified['cleaned_title'].iloc[5])\n",
    "\n",
    "cnn_modified.to_csv('../data/cleaned/clean_for_each/cnn_clean.csv', index=False, sep=',', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3156b4",
   "metadata": {},
   "source": [
    "### CLEANING KOMPAS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1418f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9627, 3)\n",
      "(4286, 2)\n"
     ]
    }
   ],
   "source": [
    "kompas_modified = modified_data['kompas_modified']\n",
    "print(cnn_modified.shape)\n",
    "\n",
    "# Remove whitespace and duplicate title value\n",
    "kompas_modified['info'] = kompas_modified['info'].str.strip()\n",
    "kompas_modified = kompas_modified.drop_duplicates(subset=['info'])\n",
    "print(kompas_modified.shape)\n",
    "\n",
    "# Re-calculate data index after remove duplicate value if any\n",
    "kompas_modified = kompas_modified.reset_index(drop=True)\n",
    "\n",
    "# Apply preprocessing function to all of the data\n",
    "kompas_modified['cleaned_info'] = kompas_modified['info'].apply(preprocess_indonesian)\n",
    "\n",
    "# Show results\n",
    "# print(\"Original:\", kompas_modified['title'].iloc[5])\n",
    "# print(\"Cleaned:\", kompas_modified['cleaned_title'].iloc[5])\n",
    "\n",
    "kompas_modified.to_csv('../data/cleaned/clean_for_each/kompas_clean.csv', index=False, sep=',', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63669ec2",
   "metadata": {},
   "source": [
    "### CLEANING TEMPO DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99100944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6591, 2)\n",
      "(6591, 2)\n"
     ]
    }
   ],
   "source": [
    "tempo_modified = modified_data['tempo_modified']\n",
    "print(tempo_modified.shape)\n",
    "\n",
    "tempo_modified['info'] = tempo_modified['info'].str.strip()\n",
    "tempo_modified = tempo_modified.drop_duplicates(subset=['info'])\n",
    "print(tempo_modified.shape)\n",
    "\n",
    "tempo_modified = tempo_modified.reset_index(drop=True)\n",
    "\n",
    "tempo_modified['cleaned_info'] = tempo_modified['info'].apply(preprocess_indonesian)\n",
    "\n",
    "tempo_modified.to_csv('../data/cleaned/clean_for_each/tempo_clean.csv', index=False, sep=',', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a17a6a",
   "metadata": {},
   "source": [
    "### CLEANING TURNBACKHOAX DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbd448a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for knowing all of the categories in the beginning of string\n",
    "def extract_prefix(text):\n",
    "    # Check the pattern [KATEGORI], (KATEGORI), or KATEGORI\n",
    "    match = re.search(r'^(?:\\[([^\\]]+)\\]|\\(([^)]+)\\)|([^:\\s]+):)', str(text))\n",
    "    if match:\n",
    "        # Return back the match group (ignore the NONE)\n",
    "        return next(item for item in match.groups() if item is not None)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f66ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for removing all of the prefix\n",
    "def remove_prefixes(text, prefix_list):\n",
    "    text = text.strip()\n",
    "    for prefix in prefix_list:\n",
    "        if prefix in text:\n",
    "            return text[len(prefix)+2:].strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63490bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SALAH' 'FALSE'\n",
      " 'SALAH) Akun Whatsapp Kepala BKPSDM Kab Tangerang Hendar Herawan “+6285841645716”hasil periksa fakta Rahmah an. \\nAkun palsu. BKPSDM Kab Tangerang tidak pernah menawarkan/menjanjikan apapun atau melakukan pungutan apapun terkait pelayanan kepegawaian di BKPSDM Kab Tangerang.\\nSelengkapnya di penjelasan.\\n===[KATEGORI'\n",
      " 'SALAH) Akun Whatsapp Bupati Bojonegoro Anna Mu’awanah “+628133728109”Hasil periksa fakta Rahmah an. \\nAkun palsu. Kepala Bidang Pengelolaan Informasi dan Komunikasi Publik (PIKP) Dinas Kominfo Kabupaten Bojonegoro, Nanang Dwi Cahyono, memastikan bahwa nomor yang beredar bukan milik Bupati Anna.\\nSelengkapnya di penjelasan.\\n===[KATEGORI'\n",
      " 'SALAH) Akun Whatsapp Kadis Kominfosanti Ketut Suwarmawan Kabupaten Buleleng “+6281324422253”Hasil periksa fakta Ari Dwi Prasetyo.\\nAkun palsu. Melansir Cirt.bulelengkab.go.id, menerangkan bahwa akun Whatsapp dengan nomor tersebut bukanlah akun Whatsapp resmi dari Kadis Komunikasi, Informatika, Persandian, dan Statistik Kabupaten Buleleng.\\nSelengkapnya di penjelasan.\\n===[KATEGORI'\n",
      " 'SALAH) Akun Whatsapp Sekretaris Daerah Kabupaten Aceh Utara A. Murtala “+6281250401484”hasil periksa fakta Rahmah an. \\nAkun palsu. Pihak Diskominfo Aceh Utara memastikan bahwa nomor tersebut bukan milik Sekda A. Murtala.\\nSelengkapnya di penjelasan.\\n===[KATEGORI'\n",
      " 'SALAH) Akun Whatsapp Wakil Wali Kota Cirebon Eti Herawati “+6285837232955”hasil periksa fakta Rahmah an. \\nAkun palsu. Melalui akun Instagram resminya, Wakil Wali Kota Cirebon, Eti Herawati menjelaskan bahwa akun yang beredar adalah bukan\\nSelengkapnya di penjelasan.\\n===[KATEGORI'\n",
      " 'SALAH) Akun Whatsapp Ketua PTUN Palangkaraya Nenny Frantika “+6281366865116” Meminta Sejumlah Danahasil periksa fakta Rahmah an.\\nAkun palsu. Akun Instagram PTUN Palangkaraya @ptun_palangkaraya mengklarifikasi bahwa akun Whatsapp yang beredar bukan milik Ibu Nenny Frantika.\\nSelengkapnya di penjelasan. \\n===\\n[KATEGORI'\n",
      " 'SALAH) Akun Whatsapp Bupati Grobogan Sri Sumarni “+6282132330593”hasil periksa fakta Rahmah an.\\nAkun palsu. Akun Instagram purwodadi_ku mengonfirmasi akun Whatsapp yang beredar bukan milik Bupati Grobogan Sri Sumarni.\\nSelengkapnya di penjelasan\\n===\\n[KATEGORI'\n",
      " 'SALAH) Lowongan Kerja JNE Tegalhasil periksa fakta Rahmah an.\\nLowongan palsu. Akun Instagram JNE Tegal @jne.tegal menegaskan lowongan tersebut adalah hoaks. Segala informasi mengenai JNE hanya bisa didapatkan melalui akun media social resmi JNE @jne_id dan @jne_tegal (Instagram).\\nSelengkapnya di penjelasan\\n===\\n[KATEGORI'\n",
      " 'UPDATE' 'Top 5' 'BERITA' 'ACARA' 'DOKUMENTASI' 'EVENT' 'RILIS PERS'\n",
      " 'EDUKASI' 'KLARIFIKASI' 'CLARIFICATION' 'KOREKSI' 'BENAR' 'Klarifikasi'\n",
      " 'Facebook' 'Cek Fakta' 'CekFakta' 'HOAX' 'DISINFORMASI' 'ISU'\n",
      " 'MISINFORMASI' 'Isu' 'DISINFORMASI + HASUT' 'Admin Post' 'HOAKS'\n",
      " 'Siaran Pers' 'Benar' 'DISINFORMASI+HASUT' 'HOAX/FITNAH'\n",
      " 'INFORMASI & EDUKASI' 'INFORMASI' 'INFO' 'HOAX + HASUT' 'HASUT+FITNAH'\n",
      " 'Hoax' 'SCAM' 'FITNAH' 'DISINFORMASI, HASUT' 'HASUT'\n",
      " 'DISINFORMASI & HASUT' 'DISINFORMASI/MISINFORMASI'\n",
      " 'HOAX, HASUT, & PROVOKASI' 'Disinformasi' 'DISINFORMASI+FITNAH'\n",
      " 'Misinformasi' 'HOAX+HASUT' 'EDUKASI, BERITA' 'FITNAH+HOAX'\n",
      " 'DISINFORMASI+FRAMING' 'FRAMING' 'HOAX/HASUT' 'FITNAH+HASUT'\n",
      " 'BERITA, EDUKASI' 'HOAX + LOGICAL FALLACY' 'HOAX KILLS' 'HOAX+FITNAH'\n",
      " 'FITNAH / HASUT' 'EVENTS' 'Campuran; Disinformasi, Hasut, & Fakta'\n",
      " 'HOAX, HASUT' 'INFORMASI, EDUKASI' 'KLARIFIKASI, EDUKASI' 'EDUKASI, HOAX'\n",
      " 'EDUKASI,HOAX' 'FAKTA']\n"
     ]
    }
   ],
   "source": [
    "turnbackhoax_modified = modified_data['turnbackhoax_modified']\n",
    "\n",
    "# find the prefix in all of the strings and remove them from strings\n",
    "prefixes = turnbackhoax_modified['info'].apply(extract_prefix).dropna().unique()\n",
    "print(prefixes)\n",
    "turnbackhoax_modified['info'] = turnbackhoax_modified['info'].apply(lambda data: remove_prefixes(data, prefixes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc8513ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "turnbackhoax_modified['info'] = turnbackhoax_modified['info'].str.strip()\n",
    "turnbackhoax_modified = turnbackhoax_modified.drop_duplicates(subset=['info'])\n",
    "turnbackhoax_modified = turnbackhoax_modified.reset_index(drop=True)\n",
    "# print(turnbackhoax_modified['hoax'].value_counts())\n",
    "\n",
    "turnbackhoax_modified['cleaned_info'] = turnbackhoax_modified['info'].apply(preprocess_indonesian)\n",
    "# print(\"Original:\", turnbackhoax_modified['title'].iloc[9])\n",
    "# print(\"Cleaned:\", turnbackhoax_modified['cleaned_title'].iloc[9])\n",
    "# turnbackhoax_modified\n",
    "\n",
    "turnbackhoax_modified.to_csv('../data/cleaned/clean_for_each/turnbackhoax_clean.csv', index=False, sep=',', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93fc30b",
   "metadata": {},
   "source": [
    "### MERGE DATA AND SAVE INTO DIRECTORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a5f3443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20504, 2)\n"
     ]
    }
   ],
   "source": [
    "true_data = pd.concat([cnn_modified, kompas_modified, tempo_modified], ignore_index=True)\n",
    "true_data['info'] = true_data['cleaned_info']\n",
    "true_data = true_data.drop('cleaned_info', axis=1)\n",
    "\n",
    "true_data_shuffled = shuffle(true_data).reset_index(drop=True)\n",
    "print(true_data_shuffled.shape)\n",
    "\n",
    "true_data_shuffled.to_csv('../data/cleaned/clean_part_merged/true_data.csv', index=False, sep=',', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60fe19df",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_data = turnbackhoax_modified\n",
    "false_data['info'] = false_data['cleaned_info']\n",
    "false_data = false_data.drop('cleaned_info', axis=1)\n",
    "\n",
    "false_data.to_csv('../data/cleaned/clean_part_merged/false_data.csv', index=False, sep=',', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "779d548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge true (fact) and false (hoax) data\n",
    "all_clean_data = pd.concat([true_data, false_data], ignore_index=True)\n",
    "all_clean_data_shuffled = shuffle(all_clean_data).reset_index(drop=True)\n",
    "\n",
    "all_clean_data_shuffled.to_csv('../data/cleaned/clean_all_merged/clean_data_all.csv', index=False, sep=',', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_PROJECT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
